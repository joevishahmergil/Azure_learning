import os
import time
import uuid
import traceback
import fitz  # PyMuPDF
import time
import pickle
from langchain.vectorstores import Chroma
from langchain.storage import InMemoryStore
from langchain.schema.document import Document
from langchain_openai import AzureOpenAIEmbeddings
from langchain.chat_models import AzureChatOpenAI
from langchain.retrievers.multi_vector import MultiVectorRetriever
from langchain.chat_models import AzureChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from unstructured.partition.pdf import partition_pdf


# Azure cosmosDB imports
from azure.cosmos import CosmosClient, PartitionKey
from langchain_community.vectorstores.azure_cosmos_db_no_sql import AzureCosmosDBNoSqlVectorSearch
import pickle
import os
import json
from datetime import datetime


# Azure blob store
from azure.storage.blob import BlobServiceClient


from langchain.schema import BaseStore

def setup_retriever():
    # Azure Cosmos DB configuration
    HOST = "https://daikin-test.documents.azure.com:443/"  # URI
    KEY = "GNNcKM5UXF26cvCklrK4IftkCDsihkFgsQBUyqGEte5nPsvbS5W1KoL1gMvNKLAf5Zbo8KlRQForACDbYpKaQA=="  # PRIMARY KEY
    cosmos_client = CosmosClient(HOST, KEY)
    database_name = "rag_server_20pg_30ktpm"
    container_name = "rag_server_container_20pg_30ktpm"
    # database_name = "rag_server_100pdfdb"
    # container_name = "rag_server_container_100pdf"
    partition_key = PartitionKey(path="/id")
    
    # Vector embedding policy
    vector_embedding_policy = {
        "vectorEmbeddings": [
            {
                "path": "/embedding",
                "dataType": "float32",
                "distanceFunction": "cosine",
                "dimensions": 1536,  # Adjust based on your embedding model
            }
        ]
    }
    
    # Remove the full text policy that's causing issues
    # full_text_policy = None  # Not using full text search
    
    # Indexing policy - without full text indexes
    indexing_policy = {
        "indexingMode": "consistent",
        "includedPaths": [{"path": "/*"}],
        "excludedPaths": [{"path": '/"_etag"/?'}],
        "vectorIndexes": [{"path": "/embedding", "type": "diskANN"}]
        # Removed the fullTextIndexes section
    }
    
    # Container properties
    cosmos_container_properties = {"partition_key": partition_key}
    
    # Database properties (empty dict)
    cosmos_database_properties = {}
    
    # Get Azure embeddings
    embeddings = AzureOpenAIEmbeddings(
        azure_deployment=AZURE_EMBEDDING_DEPLOYMENT,
        model=AZURE_EMBEDDING_MODEL,
        api_version=AZURE_EMBEDDING_API_VERSION,
        openai_api_type="azure",
        azure_endpoint=AZURE_EMBEDDING_ENDPOINT,
        api_key=AZURE_EMBEDDING_KEY,
    )
    
    # Create the vectorstore - with proper error handling
    try:
        # First, try to create the database if it doesn't exist
        try:
            database = cosmos_client.create_database_if_not_exists(id=database_name)
            print(f"Database '{database_name}' created or already exists")
        except Exception as db_error:
            print(f"Error creating database: {db_error}")
            database = cosmos_client.get_database_client(database_name)
        
        # Try to create the container
        try:
            container = database.create_container_if_not_exists(
                id=container_name,
                partition_key=partition_key,
                indexing_policy=indexing_policy,
                vector_embedding_policy=vector_embedding_policy
                # Removed full_text_policy
            )
            print(f"Container '{container_name}' created or already exists")
        except Exception as container_error:
            print(f"Error creating container: {container_error}")
            container = database.get_container_client(container_name)
        
        # Create the vectorstore with all required parameters
        vectorstore = AzureCosmosDBNoSqlVectorSearch(
            cosmos_client=cosmos_client,
            database_name=database_name,
            container_name=container_name,
            embedding=embeddings,
            vector_embedding_policy=vector_embedding_policy,
            indexing_policy=indexing_policy,
            cosmos_container_properties=cosmos_container_properties,
            cosmos_database_properties=cosmos_database_properties,
            # Not using full text search
            full_text_search_enabled=False
        )
        print("Successfully created AzureCosmosDBNoSqlVectorSearch instance")
        
    except Exception as e:
        print(f"Error setting up Cosmos DB: {e}")
        raise
    
    # Create a custom document store that uses Azure Blob Storage directly
    class CustomAzureBlobDocStore(BaseStore):  
        def __init__(self, connection_string, container_name):
            self.blob_service_client = BlobServiceClient.from_connection_string(connection_string)
            self.container_name = container_name
            
            # Create container if it doesn't exist
            container_client = self.blob_service_client.get_container_client(container_name)
            if not container_client.exists():
                self.blob_service_client.create_container(container_name)
                print(f"Created container: {container_name}")
            
            self.container_client = self.blob_service_client.get_container_client(container_name)

        def mget(self, keys):
            results = []
            for key in keys:
                try:
                    blob_client = self.container_client.get_blob_client(key)
                    if blob_client.exists():
                        data = blob_client.download_blob().readall()
                        try:
                            obj = pickle.loads(data)
                            results.append(obj)
                        except Exception as e:
                            print(f"Error deserializing blob {key}: {e}")
                            results.append(None)
                    else:
                        results.append(None)
                except Exception as e:
                    print(f"Error retrieving blob {key}: {e}")
                    results.append(None)
            return results

        def mset(self, key_value_pairs):
            for key, value in key_value_pairs:
                try:
                    blob_data = pickle.dumps(value)
                    blob_client = self.container_client.get_blob_client(key)
                    blob_client.upload_blob(blob_data, overwrite=True)
                except Exception as e:
                    print(f"Error storing blob {key}: {e}")

        def mdelete(self, keys):
            """Delete multiple blobs."""
            for key in keys:
                try:
                    self.container_client.delete_blob(key)
                except Exception as e:
                    print(f"Error deleting blob {key}: {e}")

        def yield_keys(self, prefix=None):
            """Yield all keys (blob names) from the container."""
            try:
                blobs = self.container_client.list_blobs(name_starts_with=prefix)
                for blob in blobs:
                    yield blob.name
            except Exception as e:
                print(f"Error listing blobs: {e}")
    
    # Create the document store with Azure Blob Storage
    # Replace with your actual Azure Blob Storage connection string
    docstore = CustomAzureBlobDocStore(
        connection_string="DefaultEndpointsProtocol=https;AccountName=daikinaiteststorage;AccountKey=2ocP7JAilXoPvD2soNVPmIsgVXN+pKtWvjoBxrA5cjoRcWO4jXPKqoC74z12PDPp7Ob6+pl2leVg+AStCKdujA==;EndpointSuffix=core.windows.net", # Connection string
        container_name="rag-server-docstore-20pg-30ktpm"  # Blob container name
        # container_name = "rag-server-docstore-100"
    )
    
    # Create and return the retriever
    return MultiVectorRetriever(
        vectorstore=vectorstore,
        docstore=docstore,
        id_key="doc_id"
    )
def get_images_base64(chunks):
    """Extracts images from CompositeElement chunks and returns them in base64 format."""
    images_b64 = []
    for chunk in chunks:
        if "CompositeElement" in str(type(chunk)):
            if hasattr(chunk.metadata, 'orig_elements'):
                for el in chunk.metadata.orig_elements:
                    if "Image" in str(type(el)) and hasattr(el.metadata, 'image_base64'):
                        images_b64.append(el.metadata.image_base64)
    return images_b64

def display_base64_image(base64_code):
    """Decodes and displays a base64-encoded image."""
    try:
        image_data = base64.b64decode(base64_code)
        display(Image(data=image_data))
    except Exception as e:
        print(f"Error displaying image: {str(e)}")

# def describe_images_with_gpt(images):
#     prompt_template = """Describe the image in detail. Make sure the dimensions and numbers of the image should be accurately returned."""
#     messages = [
#         (
#             "user",
#             [
#                 {"type": "text", "text": prompt_template},
#                 {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,{image}"}},
#             ],
#         )
#     ]
    #   llm = AzureChatOpenAI(
    #         temperature=0,
    #         max_tokens=4096,
    #         azure_endpoint=azure_endpoint,
    #         azure_deployment=azure_deployment,
    #         api_version=azure_api_version,
    #         api_key=azure_api_key
    #     )

#     prompt = ChatPromptTemplate.from_messages(messages)
#     chain = prompt | llm | StrOutputParser()
#     return chain.batch(images)
def describe_images_with_gpt(images, batch_size=5, tpm_limit=30000, tokens_per_image=800, max_retries=5, retry_delay=45):
    """
    Process images in batches to avoid hitting the rate limit.

    Args:
        images (list): List of base64 encoded image strings.
        batch_size (int): Number of images to process per batch.
        tpm_limit (int): Token limit per minute.
        tokens_per_image (int): Estimated tokens consumed per image.
        max_retries (int): Maximum number of retry attempts for rate-limited requests.
        retry_delay (int): Base delay in seconds between retries.

    Returns:
        list: List of descriptions for the images.
    """
    prompt_template = "Describe the image in detail. Make sure the dimensions and numbers of the image should be accurately returned."
    results = []
    prompt = ChatPromptTemplate.from_messages([
        ("user", [{"type": "text", "text": prompt_template},
                  {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,{image}"}}])
    ])
    
    llm = AzureChatOpenAI(
            temperature=0,
            max_tokens=4096,
            azure_endpoint=azure_endpoint,
            azure_deployment=azure_deployment,
            api_version=azure_api_version,
            api_key=azure_api_key
        )
    parser = StrOutputParser()

    for i in range(0, len(images), batch_size):
        batch = images[i:i + batch_size]
        retry_count = 0
        success = False

        while not success and retry_count <= max_retries:
            try:
                chain = prompt | llm | parser
                batch_results = chain.batch(batch)
                results.extend(batch_results)
                success = True
                print(f"Successfully processed batch {i//batch_size + 1} of {(len(images) + batch_size - 1)//batch_size}")
            except Exception as e:
                if '429' in str(e):  # Rate limit error
                    retry_count += 1
                    wait_time = retry_delay * retry_count  # Exponential backoff
                    print(f"Rate limit exceeded. Retrying batch {i//batch_size + 1} in {wait_time} seconds (Attempt {retry_count}/{max_retries})")
                    time.sleep(wait_time)
                else:
                    print(f"Error processing batch {i//batch_size + 1}: {e}")
                    break  # Exit the retry loop for non-rate-limit errors
        
        if not success:
            print(f"Failed to process batch {i//batch_size + 1} after {max_retries} retries")

        # Sleep if close to rate limit
        if (i + batch_size) * tokens_per_image >= tpm_limit * 0.9:
            time.sleep(60)  # Wait a minute before the next batch

    return results

def serialize_for_storage(obj):
    """
    Serialize any object to bytes for storage.
    """
    return pickle.dumps(obj)
def handle_azure_operation(operation_func, max_retries=5, base_delay=53, timeout=300):
    """
    Execute an Azure OpenAI operation with retry logic for rate limits and connection errors.
    
    Args:
        operation_func: Function to execute
        max_retries: Maximum number of retry attempts
        base_delay: Base delay in seconds between retries
        timeout: Maximum time in seconds to keep trying
        
    Returns:
        The result of the operation function or None if all retries fail
    """
    start_time = time.time()
    retry_count = 0
    
    while retry_count <= max_retries:
        try:
            # Attempt the operation
            result = operation_func()
            return result
        
        except Exception as e:
            # Handle rate limit errors (429)
            if '429' in str(e):
                retry_count += 1
                wait_time = base_delay * retry_count  # Exponential backoff
                
                print(f"Azure OpenAI rate limit exceeded. Retrying in {wait_time} seconds (Attempt {retry_count}/{max_retries})")
                
                # Check if we've exceeded the timeout
                if (time.time() - start_time + wait_time) > timeout:
                    print(f"Operation timed out after {timeout} seconds")
                    return None
                    
                time.sleep(wait_time)
                
            # Handle connection errors
            elif any(conn_err in str(e).lower() for conn_err in ['connection', 'timeout', 'connect']):
                retry_count += 1
                wait_time = min(base_delay * retry_count, 120)  # Cap at 2 minutes
                
                print(f"Connection error: {str(e)}. Retrying in {wait_time} seconds (Attempt {retry_count}/{max_retries})")
                
                # Check if we've exceeded the timeout
                if (time.time() - start_time + wait_time) > timeout:
                    print(f"Operation timed out after {timeout} seconds")
                    return None
                    
                time.sleep(wait_time)
                
            else:
                # For other errors, log and return None
                print(f"Error during Azure operation: {str(e)}")
                return None
    
    print(f"Operation failed after {max_retries} retries")
    return None
def index_pdf(file_path, retriever):
    """Process and index PDF content using the provided retriever.

    This function handles the extraction, summarization and vectorization of PDF content,
    including text, tables and images.

    Args:
        file_path: Path to the PDF file
        retriever: MultiVectorRetriever instance for storing document vectors
    """
    # Check if file exists before attempting to process it
    if not os.path.exists(file_path):
        print(f"File not found: {file_path}")
        return
    start_time = time.time()
    # Get number of pages in PDF
    try:
        with fitz.open(file_path) as doc:
            num_pages = len(doc)
    except Exception as e:
        print(f"Error reading PDF {file_path}: {e}")
        num_pages = "Unknown"

    print(f"Processing '{os.path.basename(file_path)}' with {num_pages} pages...")

    try:
        print(f"Starting extraction for: {os.path.basename(file_path)}")

        # Extract content from the PDF
        chunks = partition_pdf(
            filename=file_path,
            infer_table_structure=True,
            strategy="hi_res",
            extract_image_block_types=["Image"],
            image_output_dir_path=None,
            extract_image_block_to_payload=True,
            chunking_strategy="by_title",
            max_characters=10000,
            combine_text_under_n_chars=2000,
            new_after_n_chars=6000,
        )

        for i, chunk in enumerate(chunks):
            print(f"\n--- Chunk {i+1} ---")
            print(f"Type: {type(chunk)}")
            
            # If it's a dictionary (like image payloads)
            if isinstance(chunk, dict):
                for key, value in chunk.items():
                    if key == 'text' and isinstance(value, str):
                        print(f"{key}: {value[:500]}...")  # Preview only first 500 chars
                    else:
                        print(f"{key}: {str(value)[:200]}...")  # Preview other content
            else:
                # For unstructured text elements like from unstructured package
                if hasattr(chunk, 'text'):
                    print(f"Text:\n{chunk.text[:1000]}")  # Preview first 1000 characters
                if hasattr(chunk, 'metadata'):
                    print(f"Metadata: {chunk.metadata}")

        # Separate extracted elements with detailed logging
        tables, texts = [], []

        print(f"Total chunks extracted: {len(chunks)}")

        images = get_images_base64(chunks)  # Adding the missing get_images_base64 function call
        image_summaries = describe_images_with_gpt(images) if images else []

        # Debug information about chunk types
        chunk_types = set(type(chunk).__name__ for chunk in chunks)
        print(f"Chunk types found: {', '.join(chunk_types)}")

        for chunk in chunks:
            chunk_type = type(chunk).__name__

            if "Table" in chunk_type:
                if hasattr(chunk, 'metadata') and hasattr(chunk.metadata, 'text_as_html'):
                    tables.append(chunk)

            elif "CompositeElement" in chunk_type:
                # Add text if it has content
                if hasattr(chunk, 'text') and chunk.text and chunk.text.strip():
                    texts.append(chunk)

                # Extract images if available
                if hasattr(chunk, 'metadata') and hasattr(chunk.metadata, 'orig_elements'):
                    for el in chunk.metadata.orig_elements:
                        if "Image" in str(type(el)) and hasattr(el.metadata, 'image_base64'):
                            images.append(el.metadata.image_base64)

        print(f"Extracted {len(texts)} text chunks, {len(tables)} tables, and {len(images)} images")

        # Skip further processing if no content was extracted
        if not texts and not tables and not images:
            print(f"No content extracted from {file_path}. Skipping.")
            return

        # Summarize text and tables
        prompt_text = "Summarize the following content concisely:\n\n{element}"
        prompt = ChatPromptTemplate.from_template(prompt_text)


        # Initialize the Azure OpenAI model
        model = AzureChatOpenAI(
            temperature=0,
            max_tokens=4096,
            azure_endpoint=azure_endpoint,
            azure_deployment=azure_deployment,
            api_version=azure_api_version,
            api_key=azure_api_key
        )

        summarize_chain = {"element": lambda x: x} | prompt | model | StrOutputParser()

        # Process text elements with safeguards
        text_summaries = []
        if texts:
            try:
                # Extract just the text content for summarization
                text_contents = [text.text for text in texts if hasattr(text, 'text')]
                if text_contents:
                    # Define the text summarization operation
                    def summarize_texts():
                        return summarize_chain.batch(text_contents, {"max_concurrency": 3})
                    
                    # Execute with retry logic
                    text_summaries = handle_azure_operation(summarize_texts)
                    
                    # Fallback if summarization failed completely
                    if text_summaries is None:
                        print("Using fallback for text summarization")
                        text_summaries = [f"Text content from {os.path.basename(file_path)}" for _ in text_contents]
                    
                    print(f"Generated {len(text_summaries)} text summaries")
            except Exception as e:
                print(f"Error summarizing text elements: {str(e)}")
                # Fallback: use original text as summary if summarization fails
                text_summaries = [text.text for text in texts if hasattr(text, 'text')]


        # Process table elements with safeguards
        table_summaries = []
        if tables:
            try:
                table_contents = [table.metadata.text_as_html for table in tables
                                if hasattr(table, 'metadata') and hasattr(table.metadata, 'text_as_html')]
                if table_contents:
                    # Define the table summarization operation
                    def summarize_tables():
                        return summarize_chain.batch(table_contents, {"max_concurrency": 3})
                    
                    # Execute with retry logic
                    table_summaries = handle_azure_operation(summarize_tables)
                    
                    # Fallback if summarization failed completely
                    if table_summaries is None:
                        print("Using fallback for table summarization")
                        table_summaries = [f"Table from {os.path.basename(file_path)}" for _ in table_contents]
                    
                    print(f"Generated {len(table_summaries)} table summaries")
            except Exception as e:
                print(f"Error summarizing table elements: {str(e)}")
                # Fallback: use simple description as summary
                table_summaries = [f"Table from {os.path.basename(file_path)}" for _ in tables]

        # Store in retriever - with safeguards to prevent empty collections
        # Process texts
        if texts and text_summaries:
            # Create unique IDs and documents
            doc_ids = [str(uuid.uuid4()) for _ in range(len(text_summaries))]

            # Create documents with proper metadata
            docs = []
            for i, summary in enumerate(text_summaries):
                if summary and summary.strip():  # Only add non-empty summaries
                    docs.append(Document(
                        page_content=summary,
                        metadata={
                            "doc_id": doc_ids[i],
                            "source": file_path,
                            "content_type": "text"
                        }
                    ))

            # Add documents to vectorstore if we have any
            if docs:
                try:
                    # Add documents to vectorstore with retry logic
                    def add_text_docs():
                        retriever.vectorstore.add_documents(docs)
                        return True
                    
                    if handle_azure_operation(add_text_docs):
                        # Map IDs to original text chunks - WITH SERIALIZATION
                        retriever.docstore.mset(list(zip(
                            [doc.metadata["doc_id"] for doc in docs],
                            [serialize_for_storage(texts[i]) for i in range(len(docs))]
                        )))
                        print(f"Added {len(docs)} text documents from {os.path.basename(file_path)}")
                    else:
                        print(f"Failed to add text documents from {os.path.basename(file_path)}")
                        
                except Exception as e:
                    print(f"Error adding text documents to vectorstore: {str(e)}")
                    traceback.print_exc()




        # Process tables
        if tables and table_summaries:
            # Create unique IDs and documents
            table_ids = [str(uuid.uuid4()) for _ in range(len(table_summaries))]

            # Create documents with proper metadata
            table_docs = []
            for i, summary in enumerate(table_summaries):
                if summary and summary.strip():  # Only add non-empty summaries
                    table_docs.append(Document(
                        page_content=summary,
                        metadata={
                            "doc_id": table_ids[i],
                            "source": file_path,
                            "content_type": "table"
                        }
                    ))

            # Add documents to vectorstore if we have any
            if table_docs:
                try:
                    # Add documents to vectorstore with retry logic
                    def add_table_docs():
                        retriever.vectorstore.add_documents(table_docs)
                        return True
                    
                    if handle_azure_operation(add_table_docs):
                        # Map IDs to original table chunks - WITH SERIALIZATION
                        retriever.docstore.mset(list(zip(
                            [doc.metadata["doc_id"] for doc in table_docs],
                            [serialize_for_storage(tables[i]) for i in range(len(table_docs))]
                        )))
                        print(f"Added {len(table_docs)} table documents from {os.path.basename(file_path)}")
                    else:
                        print(f"Failed to add table documents from {os.path.basename(file_path)}")
                        
                except Exception as e:
                    print(f"Error adding table documents to vectorstore: {str(e)}")
                    traceback.print_exc()

        # Process images
        if images and image_summaries:
            img_ids = [str(uuid.uuid4()) for _ in range(len(image_summaries))]
            summary_img = [
                Document(page_content=summary, metadata={"doc_id": img_ids[i], "source": file_path, "content_type": "image"}) 
                for i, summary in enumerate(image_summaries)
            ]
            
            # Add image documents with retry logic
            def add_image_docs():
                retriever.vectorstore.add_documents(summary_img)
                return True
            
            if handle_azure_operation(add_image_docs):
                # Store images with serialization
                retriever.docstore.mset(list(zip(img_ids, [serialize_for_storage(img) for img in images[:len(image_summaries)]])))
                print(f"Added {len(summary_img)} image descriptions from {os.path.basename(file_path)}")
            else:
                print(f"Failed to add image documents from {os.path.basename(file_path)}")

    except Exception as e:
        print(f"Error processing {file_path}: {str(e)}")
        traceback.print_exc()

    end_time = time.time()
    processing_time = end_time - start_time
    hours, remainder = divmod(processing_time, 3600)
    minutes, seconds = divmod(remainder, 60)
    time_str = ""
    if hours > 0:
        time_str += f"{int(hours)}hrs "
    if minutes > 0 or hours > 0:
        time_str += f"{int(minutes)}mins "
    time_str += f"{seconds:.2f}sec"
    print(f"Finished processing '{os.path.basename(file_path)}' in {time_str}")
    # print(f"Finished processing '{os.path.basename(file_path)}' in {processing_time:.2f} seconds.")


def process_directory(directory_path, retriever):
    """Processes all PDFs in a directory and indexes them."""
    if not os.path.exists(directory_path):
        print(f"Directory not found: {directory_path}")
        return

    print(f"Scanning directory: {directory_path}")

    pdf_files = []
    for filename in os.listdir(directory_path):
        file_path = os.path.join(directory_path, filename)
        if filename.endswith(".pdf") and os.path.isfile(file_path):
            pdf_files.append(file_path)

    print(f"Found {len(pdf_files)} PDF files to process")

    for file_path in pdf_files:
        print(f"\nProcessing: {os.path.basename(file_path)}")
        try:
            index_pdf(file_path, retriever)
        except Exception as e:
            print(f"Failed to process {os.path.basename(file_path)}: {str(e)}")
            traceback.print_exc()

    print("\nDirectory processing complete")


retriever = setup_retriever()
file_path = "D:\\Projects\\Daikin\\Testing\\3_doc\\CDXS07LVJU Submittal Sheet.pdf"
def index_pdf(file_path):
    """Process and index PDF content using the provided retriever.

    This function handles the extraction, summarization and vectorization of PDF content,
    including text, tables and images.

    Args:
        file_path: Path to the PDF file
        retriever: MultiVectorRetriever instance for storing document vectors
    """
    # Check if file exists before attempting to process it
    if not os.path.exists(file_path):
        print(f"File not found: {file_path}")
        return
    start_time = time.time()
    # Get number of pages in PDF
    try:
        with fitz.open(file_path) as doc:
            num_pages = len(doc)
    except Exception as e:
        print(f"Error reading PDF {file_path}: {e}")
        num_pages = "Unknown"

    print(f"Processing '{os.path.basename(file_path)}' with {num_pages} pages...")


    print(f"Starting extraction for: {os.path.basename(file_path)}")

        # Extract content from the PDF
    chunks = partition_pdf(
            filename=file_path,
            infer_table_structure=True,
            strategy="hi_res",
            extract_image_block_types=["Image"],
            image_output_dir_path=None,
            extract_image_block_to_payload=True,
            chunking_strategy="by_title",
            max_characters=10000,
            combine_text_under_n_chars=2000,
            new_after_n_chars=6000,
        )
    for i, chunk in enumerate(chunks):
        print(f"\n--- Chunk {i+1} ---")
        print(f"Type: {type(chunk)}")

        # Case 1: Dictionary (likely image payload)
        if isinstance(chunk, dict):
            for key, value in chunk.items():
                print(f"{key}: {str(value)[:500]}{'...' if len(str(value)) > 500 else ''}")

        # Case 2: Text, Table, or other document element from unstructured package
        else:
            # Print element type (e.g., NarrativeText, Title, Table, etc.)
            element_type = getattr(chunk, "category", getattr(chunk, "__class__", None).__name__)
            print(f"Element Type: {element_type}")

            # Print metadata like page number and coordinates
            if hasattr(chunk, "metadata"):
                print("Metadata:")
                metadata_dict = chunk.metadata.__dict__
                for k, v in metadata_dict.items():
                    print(f"  {k}: {v}")

            # Print actual text content
            if hasattr(chunk, "text"):
                text = chunk.text.strip()
                print(f"Content:\n{text[:1000]}{'...' if len(text) > 1000 else ''}")



index_pdf(file_path)
# Example usage
directory_path = "doc"
process_directory(directory_path, retriever)
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.messages import HumanMessage
from langchain.chat_models import AzureChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from base64 import b64decode

llm = AzureChatOpenAI(
            temperature=0,
            max_tokens=4096,
            azure_endpoint=azure_endpoint,
            azure_deployment=azure_deployment,
            api_version=azure_api_version,
            api_key=azure_api_key
        )

def parse_docs(docs):
    """Split base64-encoded images and texts"""
    b64 = []
    text = []
    for doc in docs:
        # Deserialize the document if it's in bytes format
        if isinstance(doc, bytes):
            try:
                doc = pickle.loads(doc)
            except Exception as e:
                print(f"Error deserializing document: {e}")
                continue
                
        try:
            b64decode(doc)
            b64.append(doc)
        except Exception:
            text.append(doc)
    return {"images": b64, "texts": text}

def build_prompt(kwargs):
    docs_by_type = kwargs["context"]
    user_question = kwargs["question"]

    context_text = ""
    if len(docs_by_type["texts"]) > 0:
        for text_element in docs_by_type["texts"]:
            context_text += text_element.text

    # Construct prompt with context (including images)
    prompt_template = f"""
    Answer the question based only on the following context, which can include text, tables, and the below image.
    Context: {context_text}
    Question: {user_question}
    """

    prompt_content = [{"type": "text", "text": prompt_template}]

    if len(docs_by_type["images"]) > 0:
        for image in docs_by_type["images"]:
            prompt_content.append(
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image}"},
                }
            )

    return ChatPromptTemplate.from_messages(
        [
            HumanMessage(content=prompt_content),
        ]
    )

def query_ra_pipeline(retriever, query):
    # k_docs = 25  # Get more documents for better chances of finding relevant info
    
    # # Modify retriever search parameters for this query
    # if hasattr(retriever, "search_kwargs"):
    #     original_k = retriever.search_kwargs.get("k", 4)
    #     retriever.search_kwargs["k"] = k_docs
    chain = (
        {
            "context": retriever | RunnableLambda(parse_docs),
            "question": RunnablePassthrough(),
        }
        | RunnableLambda(build_prompt)
        | llm
        | StrOutputParser()
    )

    return chain.invoke(query)
# Example usage:
response = query_ra_pipeline(retriever, "For a heat pump system, what is the maximum allowable length of the pipe after the first branch in VRV W T-Series water-cooled system air conditioner?")
print(response)
# IM-KRP928
chain_with_sources = {
    "context": retriever | RunnableLambda(parse_docs),
    "question": RunnablePassthrough(),
} | RunnablePassthrough().assign(
    response=(
        RunnableLambda(build_prompt)
        | llm
        | StrOutputParser()
    )
)
response = chain_with_sources.invoke(
    "what is the recommended suction line diameter (in inches OD) for a 3-ton condensing unit with a line set length equivalent to 0-250 feet?"
)

print("Response:", response['response'])

print("\n\nContext:")
print("Texts:")
for text in response['context']['texts']:
    # Check if text is a Document object with text attribute
    if hasattr(text, 'text'):
        print(text.text)
        # Safely access metadata
        if hasattr(text, 'metadata') and hasattr(text.metadata, 'page_number'):
            print("Page number:", text.metadata.page_number)
        else:
            # Print all available metadata
            print("Metadata:", text.metadata if hasattr(text, 'metadata') else "No metadata")
    else:
        # If text is a string
        print(text)
    print("\n" + "-"*50 + "\n")

print("Images:")
if 'images' in response['context'] and response['context']['images']:
    for image in response['context']['images']:
        # Check if display_base64_image function is defined
        try:
            display_base64_image(image)
        except NameError:
            print("(Image would be displayed here)")
else:
    print("No images in context")
if not response['context']['texts'] and not response['context']['images']:
    print("LLM guess (not retrieved from your documents):")
else:
    print("Document-based answer:")

print("Response:", response['response'])
